{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ad0ede4-2046-4a2c-b8ee-010a136088cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T15:52:16.386403Z",
     "iopub.status.busy": "2025-04-12T15:52:16.386089Z",
     "iopub.status.idle": "2025-04-12T15:52:16.390750Z",
     "shell.execute_reply": "2025-04-12T15:52:16.389964Z",
     "shell.execute_reply.started": "2025-04-12T15:52:16.386381Z"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import pathlib\n",
    "import sys\n",
    "from IPython.display import clear_output, display\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64b15dd2-6f20-478c-b9c4-fc74952687a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T15:47:35.786987Z",
     "iopub.status.busy": "2025-04-12T15:47:35.786707Z",
     "iopub.status.idle": "2025-04-12T15:47:35.915740Z",
     "shell.execute_reply": "2025-04-12T15:47:35.915037Z",
     "shell.execute_reply.started": "2025-04-12T15:47:35.786971Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'yolov7' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/WongKinYiu/yolov7\n",
    "\n",
    "yolov7_path = \"/kaggle/working/yolov7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17f92570-b0e0-416a-88d1-89630368fee4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T15:51:15.448505Z",
     "iopub.status.busy": "2025-04-12T15:51:15.448223Z",
     "iopub.status.idle": "2025-04-12T15:51:15.452614Z",
     "shell.execute_reply": "2025-04-12T15:51:15.451881Z",
     "shell.execute_reply.started": "2025-04-12T15:51:15.448484Z"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Add Yolo v7 Repository to Python path\n",
    "\n",
    "pth_yolov7 = pathlib.Path(r'/home/roboticscorner/Python/yolov7')\n",
    "\n",
    "if not str(pth_yolov7) in sys.path:\n",
    "    sys.path.append(str(pth_yolov7))\n",
    "\n",
    "from utils.general import non_max_suppression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a68f9eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_video_path = \"/home/roboticscorner/Python/people_with_helmet_working_1920_1080_50fps.mp4\"\n",
    "\n",
    "weights_path = \"/home/roboticscorner/Downloads/best.pt\"\n",
    "\n",
    "Yolo_Detect_file = \"/home/roboticscorner/Python/yolov7/detect.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18eef7ae-f836-4319-bb2b-bafd66f900b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T15:51:16.235843Z",
     "iopub.status.busy": "2025-04-12T15:51:16.235250Z",
     "iopub.status.idle": "2025-04-12T15:51:16.706751Z",
     "shell.execute_reply": "2025-04-12T15:51:16.706066Z",
     "shell.execute_reply.started": "2025-04-12T15:51:16.235814Z"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6237/4073698418.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "IDetect.fuse\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/roboticscorner/.local/lib/python3.8/site-packages/torch/functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3609.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "# Initialize Torch device, model and get stride\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "model_path = weights_path\n",
    "ckpt = torch.load(model_path, map_location=device)\n",
    "model = ckpt['model'].float().fuse().eval()\n",
    "for m in model.modules():\n",
    "    if type(m) in [nn.Hardswish, nn.LeakyReLU, nn.ReLU, nn.ReLU6, nn.SiLU]:\n",
    "        m.inplace = True  # pytorch 1.7.0 compatibility\n",
    "    elif type(m) is nn.Upsample:\n",
    "        m.recompute_scale_factor = None  # torch 1.11.0 compatibility\n",
    "model.half();\n",
    "\n",
    "stride = int(model.stride.max().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d25e69dd-3da1-417b-8395-48e17ef5eeed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T15:51:19.235840Z",
     "iopub.status.busy": "2025-04-12T15:51:19.235143Z",
     "iopub.status.idle": "2025-04-12T15:51:19.240184Z",
     "shell.execute_reply": "2025-04-12T15:51:19.239609Z",
     "shell.execute_reply.started": "2025-04-12T15:51:19.235813Z"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def letterbox(im, new_width, stride):\n",
    "    \"\"\"Resizes image to new width while maintaining aspect ratio, and trims to ensure height is a multiple of stride.\"\"\"\n",
    "    new_width = int(new_width)\n",
    "    h, w = im.shape[:2]\n",
    "    r = new_width / w\n",
    "    scaled_height = int(r * h)\n",
    "    im = cv2.resize(im, (new_width, scaled_height), interpolation=cv2.INTER_LINEAR)\n",
    "    trim_rows = scaled_height % stride\n",
    "    if trim_rows != 0:\n",
    "        final_height = scaled_height - trim_rows\n",
    "        offset = trim_rows // 2\n",
    "        im = im[offset:(offset + final_height)]\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3724a889-ab51-43c0-966e-e78798fe79bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T15:51:21.400926Z",
     "iopub.status.busy": "2025-04-12T15:51:21.400344Z",
     "iopub.status.idle": "2025-04-12T15:51:21.405385Z",
     "shell.execute_reply": "2025-04-12T15:51:21.404572Z",
     "shell.execute_reply.started": "2025-04-12T15:51:21.400899Z"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def run_model(model, img, device):\n",
    "    \"\"\"Runs a PyTorch model on the input image tensor after preprocessing it.\"\"\"\n",
    "    img = np.expand_dims(img, 0)\n",
    "    img = img[:, :, :, ::-1].transpose(0, 3, 1, 2)\n",
    "    img = np.ascontiguousarray(img)\n",
    "    img = torch.from_numpy(img).to(device).half()\n",
    "    img /= 255.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        return model(img)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cdf66f1-67d6-4621-af2e-4d968132e747",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T15:51:22.164518Z",
     "iopub.status.busy": "2025-04-12T15:51:22.164024Z",
     "iopub.status.idle": "2025-04-12T15:51:22.170969Z",
     "shell.execute_reply": "2025-04-12T15:51:22.170219Z",
     "shell.execute_reply.started": "2025-04-12T15:51:22.164495Z"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_one_box(x, img, label, color):\n",
    "    \"\"\"Draws a rectangle on the input image, adds a label with the given color, and writes it on the image.\"\"\"\n",
    "    c1, c2 = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))\n",
    "    cv2.rectangle(img, c1, c2, color, thickness=1, lineType=cv2.LINE_AA)\n",
    "    t_size = cv2.getTextSize(label, 0, fontScale=1/3, thickness=1)[0]\n",
    "    c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3\n",
    "    cv2.rectangle(img, c1, c2, color, -1, cv2.LINE_AA)\n",
    "    cv2.putText(img, label, (c1[0], c1[1] - 2), 0, 1/3, [225, 255, 255], thickness=1, lineType=cv2.LINE_AA)\n",
    "\n",
    "def plot_boxes(img, pred, names, colors):\n",
    "    \"\"\"Draws rectangles and writes labels on an input image for each detection prediction from a list.\"\"\"\n",
    "    for det in pred:\n",
    "        for *xyxy, conf, cls in reversed(det):\n",
    "            label = f'{names[int(cls)]} {conf:.2f}'\n",
    "            plot_one_box(xyxy, img, label, colors[int(cls)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d8dd558-89f0-4fb2-a471-ced446c73859",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T16:00:22.426464Z",
     "iopub.status.busy": "2025-04-12T16:00:22.426159Z",
     "iopub.status.idle": "2025-04-12T16:00:22.580564Z",
     "shell.execute_reply": "2025-04-12T16:00:22.579734Z",
     "shell.execute_reply.started": "2025-04-12T16:00:22.426440Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 0: cd: yolov7: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!cd yolov7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18897503-e31b-42a4-88e9-8b930fc9be7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T16:07:36.849671Z",
     "iopub.status.busy": "2025-04-12T16:07:36.848812Z",
     "iopub.status.idle": "2025-04-12T16:08:04.602221Z",
     "shell.execute_reply": "2025-04-12T16:08:04.601435Z",
     "shell.execute_reply.started": "2025-04-12T16:07:36.849621Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(agnostic_nms=False, augment=False, classes=None, conf_thres=0.25, device='', exist_ok=False, img_size=640, iou_thres=0.45, name='video_1', no_trace=False, nosave=False, project='runs/detect', save_conf=False, save_txt=False, source='/home/roboticscorner/Downloads/14117669-uhd_3840_2160_30fps.mp4', update=False, view_img=True, weights=['/home/roboticscorner/Downloads/best.pt'])\n",
      "YOLOR 🚀 v0.1-128-ga207844 torch 2.4.1+cu121 CPU\n",
      "\n",
      "/home/roboticscorner/Python/yolov7/models/experimental.py:252: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(w, map_location=map_location)  # load\n",
      "Fusing layers... \n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "IDetect.fuse\n",
      "/home/roboticscorner/.local/lib/python3.8/site-packages/torch/functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3609.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "Model Summary: 314 layers, 36492560 parameters, 6194944 gradients, 103.2 GFLOPS\n",
      " Convert model to Traced-model... \n",
      " traced_script_module saved! \n",
      " model is traced! \n",
      "\n",
      "video 1/1 (1/900) /home/roboticscorner/Downloads/14117669-uhd_3840_2160_30fps.mp4: Done. (833.2ms) Inference, (0.3ms) NMS\n",
      "video 1/1 (2/900) /home/roboticscorner/Downloads/14117669-uhd_3840_2160_30fps.mp4: 8 helmets, Done. (663.7ms) Inference, (1.1ms) NMS\n",
      "video 1/1 (3/900) /home/roboticscorner/Downloads/14117669-uhd_3840_2160_30fps.mp4: 10 helmets, Done. (605.5ms) Inference, (1.0ms) NMS\n",
      "video 1/1 (4/900) /home/roboticscorner/Downloads/14117669-uhd_3840_2160_30fps.mp4: 1 head, 11 helmets, Done. (624.9ms) Inference, (0.8ms) NMS\n",
      "video 1/1 (5/900) /home/roboticscorner/Downloads/14117669-uhd_3840_2160_30fps.mp4: 1 head, 10 helmets, Done. (741.2ms) Inference, (2.4ms) NMS\n",
      "video 1/1 (6/900) /home/roboticscorner/Downloads/14117669-uhd_3840_2160_30fps.mp4: 10 helmets, Done. (629.6ms) Inference, (0.9ms) NMS\n",
      "video 1/1 (7/900) /home/roboticscorner/Downloads/14117669-uhd_3840_2160_30fps.mp4: 11 helmets, Done. (787.5ms) Inference, (0.8ms) NMS\n",
      "video 1/1 (8/900) /home/roboticscorner/Downloads/14117669-uhd_3840_2160_30fps.mp4: 12 helmets, Done. (697.8ms) Inference, (0.8ms) NMS\n",
      "video 1/1 (9/900) /home/roboticscorner/Downloads/14117669-uhd_3840_2160_30fps.mp4: 11 helmets, Done. (671.1ms) Inference, (0.8ms) NMS\n",
      "video 1/1 (10/900) /home/roboticscorner/Downloads/14117669-uhd_3840_2160_30fps.mp4: 11 helmets, Done. (671.8ms) Inference, (0.9ms) NMS\n",
      "video 1/1 (11/900) /home/roboticscorner/Downloads/14117669-uhd_3840_2160_30fps.mp4: 10 helmets, Done. (875.2ms) Inference, (1.6ms) NMS\n",
      "video 1/1 (12/900) /home/roboticscorner/Downloads/14117669-uhd_3840_2160_30fps.mp4: 10 helmets, Done. (883.2ms) Inference, (2.0ms) NMS\n",
      "video 1/1 (13/900) /home/roboticscorner/Downloads/14117669-uhd_3840_2160_30fps.mp4: 1 head, 11 helmets, Done. (705.2ms) Inference, (1.4ms) NMS\n",
      "video 1/1 (14/900) /home/roboticscorner/Downloads/14117669-uhd_3840_2160_30fps.mp4: 1 head, 11 helmets, Done. (633.6ms) Inference, (0.8ms) NMS\n",
      "video 1/1 (15/900) /home/roboticscorner/Downloads/14117669-uhd_3840_2160_30fps.mp4: 1 head, 10 helmets, Done. (645.5ms) Inference, (0.8ms) NMS\n",
      "video 1/1 (16/900) /home/roboticscorner/Downloads/14117669-uhd_3840_2160_30fps.mp4: 1 head, 11 helmets, Done. (681.9ms) Inference, (0.8ms) NMS\n",
      "video 1/1 (17/900) /home/roboticscorner/Downloads/14117669-uhd_3840_2160_30fps.mp4: 1 head, 11 helmets, Done. (776.7ms) Inference, (0.8ms) NMS\n",
      "video 1/1 (18/900) /home/roboticscorner/Downloads/14117669-uhd_3840_2160_30fps.mp4: 2 heads, 11 helmets, Done. (823.0ms) Inference, (0.8ms) NMS\n",
      "video 1/1 (19/900) /home/roboticscorner/Downloads/14117669-uhd_3840_2160_30fps.mp4: 2 heads, 11 helmets, Done. (795.9ms) Inference, (0.8ms) NMS\n",
      "video 1/1 (20/900) /home/roboticscorner/Downloads/14117669-uhd_3840_2160_30fps.mp4: 1 head, 12 helmets, Done. (644.3ms) Inference, (0.8ms) NMS\n",
      "video 1/1 (21/900) /home/roboticscorner/Downloads/14117669-uhd_3840_2160_30fps.mp4: 1 head, 11 helmets, Done. (657.7ms) Inference, (0.8ms) NMS\n",
      "video 1/1 (22/900) /home/roboticscorner/Downloads/14117669-uhd_3840_2160_30fps.mp4: 1 head, 11 helmets, Done. (612.7ms) Inference, (0.9ms) NMS\n",
      "video 1/1 (23/900) /home/roboticscorner/Downloads/14117669-uhd_3840_2160_30fps.mp4: 2 heads, 11 helmets, Done. (651.6ms) Inference, (1.1ms) NMS\n",
      "video 1/1 (24/900) /home/roboticscorner/Downloads/14117669-uhd_3840_2160_30fps.mp4: 2 heads, 11 helmets, Done. (601.0ms) Inference, (0.8ms) NMS\n",
      "video 1/1 (25/900) /home/roboticscorner/Downloads/14117669-uhd_3840_2160_30fps.mp4: 2 heads, 11 helmets, Done. (609.2ms) Inference, (1.2ms) NMS\n",
      "video 1/1 (26/900) /home/roboticscorner/Downloads/14117669-uhd_3840_2160_30fps.mp4: 2 heads, 12 helmets, Done. (653.3ms) Inference, (0.8ms) NMS\n",
      "video 1/1 (27/900) /home/roboticscorner/Downloads/14117669-uhd_3840_2160_30fps.mp4: 1 head, 12 helmets, Done. (772.9ms) Inference, (2.4ms) NMS\n",
      "video 1/1 (28/900) /home/roboticscorner/Downloads/14117669-uhd_3840_2160_30fps.mp4: 2 heads, 11 helmets, Done. (663.8ms) Inference, (0.8ms) NMS\n",
      "video 1/1 (29/900) /home/roboticscorner/Downloads/14117669-uhd_3840_2160_30fps.mp4: 1 head, 11 helmets, Done. (786.3ms) Inference, (1.0ms) NMS\n",
      "video 1/1 (30/900) /home/roboticscorner/Downloads/14117669-uhd_3840_2160_30fps.mp4: 1 head, 12 helmets, Done. (581.9ms) Inference, (0.8ms) NMS\n",
      "video 1/1 (31/900) /home/roboticscorner/Downloads/14117669-uhd_3840_2160_30fps.mp4: 2 heads, 15 helmets, Done. (666.5ms) Inference, (0.8ms) NMS\n",
      "video 1/1 (32/900) /home/roboticscorner/Downloads/14117669-uhd_3840_2160_30fps.mp4: 2 heads, 14 helmets, Done. (588.6ms) Inference, (1.1ms) NMS\n",
      "video 1/1 (33/900) /home/roboticscorner/Downloads/14117669-uhd_3840_2160_30fps.mp4: 2 heads, 14 helmets, Done. (578.8ms) Inference, (0.8ms) NMS\n",
      "video 1/1 (34/900) /home/roboticscorner/Downloads/14117669-uhd_3840_2160_30fps.mp4: 1 head, 12 helmets, Done. (585.0ms) Inference, (0.9ms) NMS\n",
      "video 1/1 (35/900) /home/roboticscorner/Downloads/14117669-uhd_3840_2160_30fps.mp4: 1 head, 12 helmets, Done. (607.6ms) Inference, (1.6ms) NMS\n",
      "video 1/1 (36/900) /home/roboticscorner/Downloads/14117669-uhd_3840_2160_30fps.mp4: 1 head, 12 helmets, Done. (631.8ms) Inference, (0.8ms) NMS\n",
      "video 1/1 (37/900) /home/roboticscorner/Downloads/14117669-uhd_3840_2160_30fps.mp4: 1 head, 14 helmets, Done. (622.2ms) Inference, (1.5ms) NMS\n",
      "video 1/1 (38/900) /home/roboticscorner/Downloads/14117669-uhd_3840_2160_30fps.mp4: 1 head, 12 helmets, Done. (683.3ms) Inference, (0.8ms) NMS\n",
      "video 1/1 (39/900) /home/roboticscorner/Downloads/14117669-uhd_3840_2160_30fps.mp4: 1 head, 13 helmets, Done. (918.7ms) Inference, (1.4ms) NMS\n",
      "video 1/1 (40/900) /home/roboticscorner/Downloads/14117669-uhd_3840_2160_30fps.mp4: 1 head, 13 helmets, Done. (823.5ms) Inference, (1.1ms) NMS\n",
      "video 1/1 (41/900) /home/roboticscorner/Downloads/14117669-uhd_3840_2160_30fps.mp4: 1 head, 13 helmets, Done. (789.5ms) Inference, (0.8ms) NMS\n",
      "video 1/1 (42/900) /home/roboticscorner/Downloads/14117669-uhd_3840_2160_30fps.mp4: 1 head, 13 helmets, Done. (786.9ms) Inference, (2.0ms) NMS\n",
      "video 1/1 (43/900) /home/roboticscorner/Downloads/14117669-uhd_3840_2160_30fps.mp4: 1 head, 11 helmets, Done. (614.3ms) Inference, (1.6ms) NMS\n",
      "video 1/1 (44/900) /home/roboticscorner/Downloads/14117669-uhd_3840_2160_30fps.mp4: 1 head, 11 helmets, Done. (669.9ms) Inference, (0.8ms) NMS\n",
      "video 1/1 (45/900) /home/roboticscorner/Downloads/14117669-uhd_3840_2160_30fps.mp4: 1 head, 12 helmets, Done. (841.4ms) Inference, (0.8ms) NMS\n",
      "video 1/1 (46/900) /home/roboticscorner/Downloads/14117669-uhd_3840_2160_30fps.mp4: 2 heads, 12 helmets, Done. (657.2ms) Inference, (2.5ms) NMS\n",
      "video 1/1 (47/900) /home/roboticscorner/Downloads/14117669-uhd_3840_2160_30fps.mp4: 1 head, 11 helmets, Done. (722.0ms) Inference, (1.6ms) NMS\n",
      "video 1/1 (48/900) /home/roboticscorner/Downloads/14117669-uhd_3840_2160_30fps.mp4: 1 head, 11 helmets, Done. (807.2ms) Inference, (0.8ms) NMS\n",
      "video 1/1 (49/900) /home/roboticscorner/Downloads/14117669-uhd_3840_2160_30fps.mp4: 1 head, 13 helmets, Done. (866.0ms) Inference, (0.9ms) NMS\n",
      "video 1/1 (50/900) /home/roboticscorner/Downloads/14117669-uhd_3840_2160_30fps.mp4: 1 head, 11 helmets, Done. (766.4ms) Inference, (0.9ms) NMS\n",
      "video 1/1 (51/900) /home/roboticscorner/Downloads/14117669-uhd_3840_2160_30fps.mp4: 2 heads, 11 helmets, Done. (1040.1ms) Inference, (1.1ms) NMS\n",
      "video 1/1 (52/900) /home/roboticscorner/Downloads/14117669-uhd_3840_2160_30fps.mp4: 1 head, 10 helmets, Done. (748.1ms) Inference, (1.7ms) NMS\n",
      "video 1/1 (53/900) /home/roboticscorner/Downloads/14117669-uhd_3840_2160_30fps.mp4: 1 head, 10 helmets, Done. (876.4ms) Inference, (0.9ms) NMS\n",
      "video 1/1 (54/900) /home/roboticscorner/Downloads/14117669-uhd_3840_2160_30fps.mp4: 1 head, 13 helmets, Done. (848.3ms) Inference, (0.8ms) NMS\n",
      "video 1/1 (55/900) /home/roboticscorner/Downloads/14117669-uhd_3840_2160_30fps.mp4: 1 head, 12 helmets, Done. (855.3ms) Inference, (1.2ms) NMS\n",
      "video 1/1 (56/900) /home/roboticscorner/Downloads/14117669-uhd_3840_2160_30fps.mp4: 2 heads, 14 helmets, Done. (859.4ms) Inference, (1.0ms) NMS\n",
      "video 1/1 (57/900) /home/roboticscorner/Downloads/14117669-uhd_3840_2160_30fps.mp4: 13 helmets, Done. (1113.7ms) Inference, (0.8ms) NMS\n",
      "video 1/1 (58/900) /home/roboticscorner/Downloads/14117669-uhd_3840_2160_30fps.mp4: "
     ]
    }
   ],
   "source": [
    "\n",
    "!python3 /home/roboticscorner/Python/yolov7/detect.py --source /home/roboticscorner/Downloads/14117669-uhd_3840_2160_30fps.mp4 --weights /home/roboticscorner/Downloads/best.pt --name video_1 --view-img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f553d43-76da-45ab-a56e-5d900668e91d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T15:54:43.333787Z",
     "iopub.status.busy": "2025-04-12T15:54:43.333473Z",
     "iopub.status.idle": "2025-04-12T15:59:47.146475Z",
     "shell.execute_reply": "2025-04-12T15:59:47.145375Z",
     "shell.execute_reply.started": "2025-04-12T15:54:43.333762Z"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set the input image size and enable benchmark mode for CuDNN to speed up inference.\n",
    "imgsz = 640\n",
    "cudnn.benchmark = True\n",
    "\n",
    "# Get the class names for the model and generate random colors for drawing boxes on the image.\n",
    "names = model.names\n",
    "colors = [[random.randint(0, 255) for _ in range(3)] for _ in names]\n",
    "# Open the default camera for capturing video.\n",
    "cap = cv2.VideoCapture('/home/roboticscorner/Python/people_with_helmet_working_1920_1080_50fps.mp4')\n",
    "\n",
    "# Loop until the camera is closed.\n",
    "try:\n",
    "    while cap.isOpened():\n",
    "        # Read a frame from the camera and ensure successfully read\n",
    "        ret, im0 = cap.read()\n",
    "        assert ret, \"Failed to read\"\n",
    "\n",
    "        # Resize and pad the image to the specified size while maintaining the aspect ratio.\n",
    "        img = letterbox(im0, imgsz, stride)\n",
    "        \n",
    "        # Run the model on the preprocessed image.\n",
    "        pred = run_model(model, img, device)\n",
    "        \n",
    "        # Perform non-maximum suppression to remove overlapping boxes.\n",
    "        pred = non_max_suppression(pred)\n",
    "        \n",
    "        # Draw the boxes on the image and display it.\n",
    "        plot_boxes(img, pred, names, colors)\n",
    "        #cv2.imshow(\"YOLO v7 Demo\", img)\n",
    "        # Show frame\n",
    "        frame_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        clear_output(wait=True)\n",
    "        plt.imshow(frame_rgb)\n",
    "        plt.axis('off')\n",
    "        display(plt.gcf())\n",
    "\n",
    "        #time.sleep(0.005)  # Optional delay\n",
    "        \n",
    "        # Exit the loop if the user presses the 'q' key.\n",
    "        # if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        #     break\n",
    "finally:\n",
    "    # Release the camera and close all windows.\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac6d554-e5f6-4df5-8075-e865f3ea5289",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7066219,
     "sourceId": 11300577,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7124785,
     "sourceId": 11379369,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
